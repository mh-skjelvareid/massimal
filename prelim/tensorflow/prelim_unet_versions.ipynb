{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5ffe4b-e78c-491d-88e8-1c4e116ee67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a7cb27-5852-4cc3-a87c-dcdc5599c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_layer(resampling_type,\n",
    "            filter_channels, \n",
    "            kernel_size, \n",
    "            resampling_factor = 2,\n",
    "            name=None,\n",
    "            initializer_mean = 0.0,\n",
    "            initializer_std = 0.02,\n",
    "            apply_batchnorm = True,\n",
    "            apply_dropout = False,\n",
    "            dropout_rate = 0.5):\n",
    "    \n",
    "    \"\"\" Spatial resampling 2D convolutional layer\n",
    "    \n",
    "    # Input parameters:\n",
    "    resampling_type:    'downsample' (convolution) or 'upsample' (transpose convolution)\n",
    "    filter_channels:    Number of filters / \"depth\" of output\n",
    "                        For images, this corresponds to number of color / wavelength channels\n",
    "    kernel_size:        Spatial size of convolutional kernel\n",
    "                        For images, if kernel_size = 3, each filter processes a 3x3 pixel neighborhood\n",
    "                        \n",
    "    # Notes\n",
    "    - Based on TF example pix2pix: https://www.tensorflow.org/tutorials/generative/pix2pix\n",
    "    \"\"\"\n",
    "    # Validate resampling layer type\n",
    "    if resampling_type not in ['downsample','upsample']:\n",
    "        raise ValueError(f\"{resampling_type} is not a valid resampling type.\")\n",
    "    \n",
    "    # Create kernel initializer for normally distributed random numbers\n",
    "    initializer = tf.random_normal_initializer(\n",
    "        mean=initializer_mean, stddev=initializer_std)                    \n",
    "    \n",
    "    # Initialize as sequential (stack of layers)\n",
    "    resamp_layer = tf.keras.Sequential(name=name)\n",
    "    \n",
    "    # Add 2D convolutional layer\n",
    "    if resampling_type == 'downsample':\n",
    "        resamp_layer.add(\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filter_channels, \n",
    "                kernel_size, \n",
    "                strides=resampling_factor, \n",
    "                padding='same',                          \n",
    "                kernel_initializer=initializer, \n",
    "                use_bias=not(apply_batchnorm)))   \n",
    "    else:\n",
    "        resamp_layer.add(\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filter_channels, \n",
    "                kernel_size, \n",
    "                strides=resampling_factor, \n",
    "                padding='same',\n",
    "                kernel_initializer=initializer,\n",
    "                use_bias=not(apply_batchnorm)))\n",
    "\n",
    "    # Add (optional) batch normalization layer\n",
    "    if apply_batchnorm:\n",
    "        resamp_layer.add(tf.keras.layers.BatchNormalization())                \n",
    "\n",
    "    # Add (optional) dropout layer\n",
    "    if apply_dropout:\n",
    "        resamp_layer.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add activation layer\n",
    "    if resampling_type == 'downsample':\n",
    "        resamp_layer.add(tf.keras.layers.LeakyReLU()) \n",
    "    else:\n",
    "        resamp_layer.add(tf.keras.layers.ReLU()) \n",
    "\n",
    "    return resamp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a6eb57d-1269-461e-af98-d8e33554fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_channels, output_channels, first_layer_channels, depth, \n",
    "         model_name=None, flip_aug=True, trans_aug=False, \n",
    "         apply_batchnorm = True, apply_dropout = False):\n",
    "    \"\"\" Simple encoder-decoder U-Net architecture\n",
    "    \n",
    "    # Arguments:\n",
    "    input_channels:         Number of channels in input image\n",
    "    output_channels:        Number of classes (including background) to segment between\n",
    "    first_layer_channels:   Number of channels in first downsampling layer\n",
    "                            Each consecutive downsampling layer doubles the number of channels\n",
    "                            In upsampling, each layer halves the number of channels\n",
    "    \n",
    "    # Keyword arguments:\n",
    "    model_name:               Name of model\n",
    "    flip_aug:           If true, a RandomFlip augmentation layer is included\n",
    "                        before the first downsampling layer\n",
    "    trans_aug:          If true, a RandomTranslation augmentation layer with \n",
    "                        height and width factor of 20% is included\n",
    "                        before the first downsampling layer\n",
    "    apply_batchnorm:    If (boolean) scalar, indicate whether to use batch normalization\n",
    "                        in all downsampling / upsampling layers\n",
    "                        If tuple of booleans (length equal to total number of \n",
    "                        downsampling / upsampling layers), indicate use of batch noarmalization\n",
    "                        for each layer\n",
    "    apply_dropout:      If (boolean) scalar, indicate whether to use dropout (rate 0.5)\n",
    "                        in all downsampling / upsampling layers.\n",
    "                        If tuple of booleans (length equal to total number of \n",
    "                        downsampling / upsampling layers), indicate use of dropout\n",
    "                        for each layer\n",
    "                        \n",
    "    # Outputs:\n",
    "    model:              Keras U-Net model\n",
    "    \n",
    "    # Notes:\n",
    "    - Based on TF tutorial: https://www.tensorflow.org/tutorials/images/segmentation\n",
    "\n",
    "    \"\"\"\n",
    "    resamp_kernel_size = 4\n",
    "    \n",
    "    # Create vectors for batchnorm / dropout booleans if scalar\n",
    "    if not isinstance(apply_batchnorm,Iterable):\n",
    "        apply_batchnorm = [apply_batchnorm for _ in range(depth*2)]\n",
    "\n",
    "    if not isinstance(apply_dropout,Iterable):\n",
    "        apply_dropout = [apply_dropout for _ in range(depth*2)]\n",
    "        \n",
    "    \n",
    "    # Define input\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, input_channels],name='input_image')   # Using None to signal variable image width and height (Ny,Nx,3)\n",
    "    x = inputs    # x used as temparary variable for data flowing between layers\n",
    "        \n",
    "    # Add augmentation layer(s)\n",
    "    if flip_aug or trans_aug:\n",
    "        aug_layer = tf.keras.Sequential(name='augmentation')\n",
    "        if flip_aug:\n",
    "            aug_layer.add(tf.keras.layers.RandomFlip())\n",
    "        if trans_aug:\n",
    "            aug_layer.add(tf.keras.layers.RandomTranslation(height_factor=0.2,width_factor=0.2))\n",
    "        x = aug_layer(x)\n",
    "\n",
    "    # Add initial convolution layer with same resolution as input image\n",
    "    x = tf.keras.layers.Conv2D(first_layer_channels,kernel_size=3,padding='same', name = 'initial_convolution')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    \n",
    "    # Define downsampling layers\n",
    "    down_stack = []\n",
    "    nchannels_downsamp = [first_layer_channels*(2**(i+1)) for i in range(depth)]\n",
    "    names_downsamp = [f'downsamp_factor_{(i+1)**2}' for i in range(depth)]   \n",
    "    for channels, name, batchnorm, dropout in zip(nchannels_downsamp,names_downsamp,apply_batchnorm[0:depth],apply_dropout[0:depth]):\n",
    "        down_stack.append(resampling_layer('downsample',\n",
    "                                           channels,\n",
    "                                           resamp_kernel_size,\n",
    "                                           name = name,\n",
    "                                           apply_batchnorm=batchnorm,\n",
    "                                           apply_dropout=dropout))\n",
    "\n",
    "    # Define upsampling layers\n",
    "    up_stack = []\n",
    "    nchannels_upsamp = [first_layer_channels*(2**i) for i in range(depth,-1)]\n",
    "    names_upsamp = [f'upsamp_factor_{i**2}' for i in range(depth-1,-1)]   \n",
    "    for channels, name, batchnorm, dropout in zip(nchannels_upsamp,names_upsamp,apply_batchnorm[depth:], apply_dropout[depth:]):\n",
    "        up_stack.append(resampling_layer('upsample',\n",
    "                                         channels,\n",
    "                                         resamp_kernel_size,\n",
    "                                         name = name,\n",
    "                                         apply_batchnorm=batchnorm,\n",
    "                                         apply_dropout=dropout))    \n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = [x]                   # Add output from first layer (before downsampling) to skips list\n",
    "    for down in down_stack:\n",
    "        x = down(x)               # Run input x through layer, then set x equal to output\n",
    "        skips.append(x)           # Add layer output to skips list\n",
    "\n",
    "    skips = reversed(skips[:-1])  # Reverse list, and don't include skip for last layer (\"bottom of U\") \n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)                                     # Run input x through layer, then set x to output\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])  # Stack layer output together with skip connection (downsampling layer output with same resolution)\n",
    "    \n",
    "    # Final layer\n",
    "    last = tf.keras.layers.Conv2D(output_channels, \n",
    "                                  filters = 3,\n",
    "                                  padding='same',\n",
    "                                  activation='softmax',\n",
    "                                  name='classification')    \n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x,name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3539b15b-b6bf-47d6-9b2d-b3e944d1b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 10\n",
    "output_channels = 4\n",
    "first_layer_channels = 64\n",
    "depth = 2\n",
    "model_name='my_unet' \n",
    "flip_aug=True \n",
    "trans_aug=False \n",
    "apply_batchnorm = True \n",
    "apply_dropout = False\n",
    "\n",
    "resamp_kernel_size = 4\n",
    "\n",
    "# Create vectors for batchnorm / dropout booleans if scalar\n",
    "if not isinstance(apply_batchnorm,Iterable):\n",
    "    apply_batchnorm = [apply_batchnorm for _ in range(depth*2)]\n",
    "\n",
    "if not isinstance(apply_dropout,Iterable):\n",
    "    apply_dropout = [apply_dropout for _ in range(depth*2)]\n",
    "\n",
    "\n",
    "# Define input\n",
    "inputs = tf.keras.layers.Input(shape=[None, None, input_channels],name='input_image')   # Using None to signal variable image width and height (Ny,Nx,3)\n",
    "x = inputs    # x used as temparary variable for data flowing between layers\n",
    "\n",
    "# Add augmentation layer(s)\n",
    "if flip_aug or trans_aug:\n",
    "    aug_layer = tf.keras.Sequential(name='augmentation')\n",
    "    if flip_aug:\n",
    "        aug_layer.add(tf.keras.layers.RandomFlip())\n",
    "    if trans_aug:\n",
    "        aug_layer.add(tf.keras.layers.RandomTranslation(height_factor=0.2,width_factor=0.2))\n",
    "    x = aug_layer(x)\n",
    "\n",
    "# Add initial convolution layer with same resolution as input image\n",
    "x = tf.keras.layers.Conv2D(first_layer_channels,kernel_size=3,padding='same', name = 'initial_convolution',activation='relu')(x)\n",
    "\n",
    "\n",
    "# Define downsampling layers\n",
    "down_stack = []\n",
    "nchannels_downsamp = [first_layer_channels*(2**(i+1)) for i in range(depth)]\n",
    "names_downsamp = [f'downsamp_factor_{(2**(i+1))}' for i in range(depth)]  \n",
    "for channels, name, batchnorm, dropout in zip(nchannels_downsamp,names_downsamp,apply_batchnorm[0:depth],apply_dropout[0:depth]):\n",
    "    down_stack.append(resampling_layer('downsample',\n",
    "                                       channels,\n",
    "                                       resamp_kernel_size,\n",
    "                                       name = name,\n",
    "                                       apply_batchnorm=batchnorm,\n",
    "                                       apply_dropout=dropout))\n",
    "\n",
    "# Define upsampling layers\n",
    "up_stack = []\n",
    "nchannels_upsamp = [first_layer_channels*(2**i) for i in range(depth-1,-1,-1)]\n",
    "names_upsamp = [f'upsamp_factor_{2**i}' for i in range(depth-1,-1,-1)]   \n",
    "for channels, name, batchnorm, dropout in zip(nchannels_upsamp,names_upsamp,apply_batchnorm[depth:], apply_dropout[depth:]):\n",
    "    up_stack.append(resampling_layer('upsample',\n",
    "                                     channels,\n",
    "                                     resamp_kernel_size,\n",
    "                                     name = name,\n",
    "                                     apply_batchnorm=batchnorm,\n",
    "                                     apply_dropout=dropout))    \n",
    "\n",
    "# Downsampling through the model\n",
    "skips = [x]                   # Add output from first layer (before downsampling) to skips list\n",
    "for down in down_stack:\n",
    "    x = down(x)               # Run input x through layer, then set x equal to output\n",
    "    skips.append(x)           # Add layer output to skips list\n",
    "\n",
    "skips = reversed(skips[:-1])  # Reverse list, and don't include skip for last layer (\"bottom of U\") \n",
    "\n",
    "# Upsampling and establishing the skip connections\n",
    "for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)                                     # Run input x through layer, then set x to output\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])  # Stack layer output together with skip connection (downsampling layer output with same resolution)\n",
    "\n",
    "# Final layer\n",
    "last = tf.keras.layers.Conv2D(output_channels, \n",
    "                              kernel_size = 3,\n",
    "                              padding='same',\n",
    "                              activation='softmax',\n",
    "                              name='classification')    \n",
    "x = last(x)\n",
    "\n",
    "model =  tf.keras.Model(inputs=inputs, outputs=x,name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6d1d7c3-4042-475b-b311-00bbf4368db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 64]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nchannels_upsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "093560de-55b4-44b4-8496-dd7ad3c2f788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['downsamp_factor_4', 'downsamp_factor_8']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'downsamp_factor_{(2**(i+2))}' for i in range(depth)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a8b6876-5b05-4ac4-9d5e-f91829107143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_unet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, None, None,  0           []                               \n",
      "                                 10)]                                                             \n",
      "                                                                                                  \n",
      " augmentation (Sequential)      (None, None, None,   0           ['input_image[0][0]']            \n",
      "                                10)                                                               \n",
      "                                                                                                  \n",
      " initial_convolution (Conv2D)   (None, None, None,   5824        ['augmentation[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " downsamp_factor_2 (Sequential)  (None, None, None,   131584     ['initial_convolution[0][0]']    \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " downsamp_factor_4 (Sequential)  (None, None, None,   525312     ['downsamp_factor_2[0][0]']      \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " upsamp_factor_2 (Sequential)   (None, None, None,   524800      ['downsamp_factor_4[0][0]']      \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, None, None,   0           ['upsamp_factor_2[0][0]',        \n",
      "                                256)                              'downsamp_factor_2[0][0]']      \n",
      "                                                                                                  \n",
      " upsamp_factor_1 (Sequential)   (None, None, None,   262400      ['concatenate_16[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, None, None,   0           ['upsamp_factor_1[0][0]',        \n",
      "                                128)                              'initial_convolution[0][0]']    \n",
      "                                                                                                  \n",
      " classification (Conv2D)        (None, None, None,   4612        ['concatenate_17[0][0]']         \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,454,532\n",
      "Trainable params: 1,453,380\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d82011-b7e6-47b5-bed1-2b4cd53c81c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
