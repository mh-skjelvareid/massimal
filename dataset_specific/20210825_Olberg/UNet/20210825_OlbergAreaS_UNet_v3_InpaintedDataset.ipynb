{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a761031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable TensorFlow debugging info and warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 2: Info and warnings not displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fadd15a-24d2-42b0-aeb3-ed81b7d21fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add massimal tools folder to path\n",
    "import sys\n",
    "sys.path.append(\"/massimal/python/tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad290a12-04f7-41bd-bd8a-c93fe535a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "# Local imports\n",
    "import annotation, hyspec_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e229a5-368a-453e-9702-c6ac738481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPUs (in case of Tensorflow trying to use GPUs and raising errors)\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Enable eager mode\n",
    "#tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3246ab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is used\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9350636-fdc4-43a0-afd5-a6a62257365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "OUTPUT_CHANNELS = 6\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3872c4-eb98-495f-b03d-37fbb8501bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "# base_dir = pathlib.Path('/media/mha114/Massimal/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS')\n",
    "base_dir = pathlib.Path('/massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS')\n",
    "\n",
    "train_dataset_path = base_dir / '5b_Rad_Georef_SGC_PCA_TrainValSplit/Training_Tiles/Dataset'\n",
    "val_dataset_path = base_dir / '5b_Rad_Georef_SGC_PCA_TrainValSplit/Validation_Tiles/Dataset'\n",
    "unet_model_save_dir = base_dir /  'X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24'\n",
    "tensorboard_log_dir = base_dir / 'X_TensorboardLogs/InpaintedDataset_TestImages-12-17-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0f2232-2a7c-4031-a32e-1a77038239d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-a71c21342c78>:2: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "train_dataset = tf.data.experimental.load(str(train_dataset_path))\n",
    "val_dataset = tf.data.experimental.load(str(val_dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e9541c-52e7-43f1-9e53-275f2572031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tiles: 457\n",
      "Number of validation tiles: 107\n",
      "Tile data shape (PCA tiles): (128, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Get number of tiles in each dataset, and dataset shape\n",
    "n_tiles_train = train_dataset.cardinality()\n",
    "n_tiles_val = val_dataset.cardinality()\n",
    "tile_nrows,tile_ncols,tile_nchannels = train_dataset.element_spec[0].shape.as_list()\n",
    "print(f'Number of training tiles: {n_tiles_train}')\n",
    "print(f'Number of validation tiles: {n_tiles_val}')\n",
    "print(f'Tile data shape (PCA tiles): {(tile_nrows,tile_ncols,tile_nchannels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71a4cee-590e-432e-80ff-760fd139457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/tutorials/images/segmentation#optional_imbalanced_classes_and_class_weights\n",
    "def add_sample_weights(image, label, name):\n",
    "    # The weights for each class, with the constraint that:\n",
    "    #     sum(class_weights) == 1.0\n",
    "    class_weights = tf.constant([0.0, 1.0, 1.0, 1.0, 1.0, 1.0]) # Hard-coded for current dataset, zero weight for background\n",
    "    class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03de5aca-511c-453f-b18b-b7ed5e427e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training dataset (tiles are originally ordered by image) and add sample weights\n",
    "train_dataset = train_dataset.shuffle(buffer_size=n_tiles_train)\n",
    "train_dataset = train_dataset.map(add_sample_weights)\n",
    "val_dataset = val_dataset.map(add_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97304c0b-6975-47f8-95a1-d9faed71edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch datasets\n",
    "train_dataset_batch = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset_batch = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d073c44c-54f2-4d53-a404-c52a017c8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pca_tile, label_tile, weight_tile in val_dataset.take(3):\n",
    "#     plt.imshow(label_tile)\n",
    "#     plt.show()\n",
    "#     plt.imshow(weight_tile)\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57c14a2-f248-4755-96e2-1a1958a552f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, None, None,  0           []                               \n",
      "                                 8)]                                                              \n",
      "                                                                                                  \n",
      " augmentation (Sequential)      (None, None, None,   0           ['input_image[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " initial_convolution (Conv2D)   (None, None, None,   2336        ['augmentation[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " downsamp_res_1/2 (Sequential)  (None, None, None,   33024       ['initial_convolution[0][0]']    \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " upsamp_res_1/1 (Sequential)    (None, None, None,   32896       ['downsamp_res_1/2[0][0]']       \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " skipconnection_res_1/1 (Concat  (None, None, None,   0          ['upsamp_res_1/1[0][0]',         \n",
      " enate)                         64)                               'initial_convolution[0][0]']    \n",
      "                                                                                                  \n",
      " classification (Conv2D)        (None, None, None,   3462        ['skipconnection_res_1/1[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 71,718\n",
      "Trainable params: 71,526\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "unet = hyspec_cnn.unet(input_channels=tile_nchannels,\n",
    "                       output_channels=OUTPUT_CHANNELS,\n",
    "                       first_layer_channels=32,\n",
    "                       depth = 1,\n",
    "               )\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ab5a7b-f765-48d5-abfa-d585e9383b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "input_image\n",
      "----\n",
      "augmentation\n",
      "\trandom_flip\n",
      "----\n",
      "initial_convolution\n",
      "----\n",
      "downsamp_res_1/2\n",
      "\tconv2d\n",
      "\tbatch_normalization\n",
      "\tleaky_re_lu\n",
      "----\n",
      "upsamp_res_1/1\n",
      "\tconv2d_transpose\n",
      "\tbatch_normalization_1\n",
      "\tre_lu\n",
      "----\n",
      "skipconnection_res_1/1\n",
      "----\n",
      "classification\n"
     ]
    }
   ],
   "source": [
    "# Print layers with sublayers\n",
    "for layer in unet.layers:\n",
    "    print('----')\n",
    "    print(layer.name)\n",
    "    if hasattr(layer,'layers'):\n",
    "        for l in layer.layers:\n",
    "            print('\\t'+l.name)\n",
    "       # print(layer.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6bf20f0-fe27-4d87-903d-7f38451a5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "model_save_filename = str(unet_model_save_dir) + '/unet_model.epoch{epoch:02d}-loss{val_loss:.6f}-acc{val_sparse_categorical_accuracy:.3f}.hdf5'\n",
    "callbacks =[tf.keras.callbacks.ModelCheckpoint(filepath = model_save_filename,\n",
    "                                               save_best_only=True,\n",
    "                                               verbose = 1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, verbose=1),\n",
    "#             tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',factor=0.2, verbose=1),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir= tensorboard_log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "584db555-0114-4a6c-a6c8-4fad1fbd7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(optimizer=tf.keras.optimizers.RMSprop(0.0001), \n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             weighted_metrics=['sparse_categorical_accuracy'], # Need weights to ignore background\n",
    "             metrics = []) # Sparse because classes are numbered, not one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7525f-a316-4a48-ac2d-67d2754929da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1135 - sparse_categorical_accuracy: 0.5359\n",
      "Epoch 1: val_loss improved from inf to 0.10412, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch01-loss0.104118-acc0.423.hdf5\n",
      "115/115 [==============================] - 7s 38ms/step - loss: 0.1135 - sparse_categorical_accuracy: 0.5359 - val_loss: 0.1041 - val_sparse_categorical_accuracy: 0.4233 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0841 - sparse_categorical_accuracy: 0.6468\n",
      "Epoch 2: val_loss improved from 0.10412 to 0.09004, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch02-loss0.090037-acc0.499.hdf5\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0843 - sparse_categorical_accuracy: 0.6443 - val_loss: 0.0900 - val_sparse_categorical_accuracy: 0.4986 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0749 - sparse_categorical_accuracy: 0.6901\n",
      "Epoch 3: val_loss improved from 0.09004 to 0.07525, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch03-loss0.075250-acc0.641.hdf5\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0749 - sparse_categorical_accuracy: 0.6917 - val_loss: 0.0753 - val_sparse_categorical_accuracy: 0.6411 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0687 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 4: val_loss did not improve from 0.07525\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0685 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.5896 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0646 - sparse_categorical_accuracy: 0.7350\n",
      "Epoch 5: val_loss improved from 0.07525 to 0.07228, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch05-loss0.072284-acc0.707.hdf5\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0644 - sparse_categorical_accuracy: 0.7348 - val_loss: 0.0723 - val_sparse_categorical_accuracy: 0.7068 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0625 - sparse_categorical_accuracy: 0.7451\n",
      "Epoch 6: val_loss improved from 0.07228 to 0.06434, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch06-loss0.064344-acc0.746.hdf5\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0623 - sparse_categorical_accuracy: 0.7446 - val_loss: 0.0643 - val_sparse_categorical_accuracy: 0.7463 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0595 - sparse_categorical_accuracy: 0.7507\n",
      "Epoch 7: val_loss improved from 0.06434 to 0.06266, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch07-loss0.062664-acc0.750.hdf5\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.0627 - val_sparse_categorical_accuracy: 0.7503 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0571 - sparse_categorical_accuracy: 0.7708\n",
      "Epoch 8: val_loss did not improve from 0.06266\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0568 - sparse_categorical_accuracy: 0.7709 - val_loss: 0.0644 - val_sparse_categorical_accuracy: 0.7295 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0572 - sparse_categorical_accuracy: 0.7647\n",
      "Epoch 9: val_loss did not improve from 0.06266\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0572 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.0671 - val_sparse_categorical_accuracy: 0.7271 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0544 - sparse_categorical_accuracy: 0.7807\n",
      "Epoch 10: val_loss improved from 0.06266 to 0.06125, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch10-loss0.061254-acc0.753.hdf5\n",
      "115/115 [==============================] - 4s 35ms/step - loss: 0.0551 - sparse_categorical_accuracy: 0.7798 - val_loss: 0.0613 - val_sparse_categorical_accuracy: 0.7530 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0541 - sparse_categorical_accuracy: 0.7820\n",
      "Epoch 11: val_loss improved from 0.06125 to 0.05990, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch11-loss0.059898-acc0.753.hdf5\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0544 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.0599 - val_sparse_categorical_accuracy: 0.7529 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0539 - sparse_categorical_accuracy: 0.7851\n",
      "Epoch 12: val_loss improved from 0.05990 to 0.05950, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch12-loss0.059499-acc0.751.hdf5\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.7858 - val_loss: 0.0595 - val_sparse_categorical_accuracy: 0.7515 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0515 - sparse_categorical_accuracy: 0.7939\n",
      "Epoch 13: val_loss did not improve from 0.05950\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0521 - sparse_categorical_accuracy: 0.7918 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.7393 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0531 - sparse_categorical_accuracy: 0.7888\n",
      "Epoch 14: val_loss improved from 0.05950 to 0.05547, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch14-loss0.055468-acc0.778.hdf5\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.7879 - val_loss: 0.0555 - val_sparse_categorical_accuracy: 0.7783 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0506 - sparse_categorical_accuracy: 0.8015\n",
      "Epoch 15: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0508 - sparse_categorical_accuracy: 0.7999 - val_loss: 0.0614 - val_sparse_categorical_accuracy: 0.7540 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0501 - sparse_categorical_accuracy: 0.8014\n",
      "Epoch 16: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.7970 - val_loss: 0.0619 - val_sparse_categorical_accuracy: 0.7454 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0507 - sparse_categorical_accuracy: 0.7985\n",
      "Epoch 17: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.7991 - val_loss: 0.0556 - val_sparse_categorical_accuracy: 0.7716 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0502 - sparse_categorical_accuracy: 0.8016\n",
      "Epoch 18: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0505 - sparse_categorical_accuracy: 0.8002 - val_loss: 0.0559 - val_sparse_categorical_accuracy: 0.7807 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0500 - sparse_categorical_accuracy: 0.7989\n",
      "Epoch 19: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.8003 - val_loss: 0.0591 - val_sparse_categorical_accuracy: 0.7561 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0483 - sparse_categorical_accuracy: 0.8114\n",
      "Epoch 20: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 35ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.8104 - val_loss: 0.0583 - val_sparse_categorical_accuracy: 0.7548 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0473 - sparse_categorical_accuracy: 0.8106\n",
      "Epoch 21: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0470 - sparse_categorical_accuracy: 0.8125 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.7873 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0474 - sparse_categorical_accuracy: 0.8084\n",
      "Epoch 22: val_loss did not improve from 0.05547\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.8099 - val_loss: 0.0561 - val_sparse_categorical_accuracy: 0.7744 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0480 - sparse_categorical_accuracy: 0.8077\n",
      "Epoch 23: val_loss improved from 0.05547 to 0.05392, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch23-loss0.053915-acc0.781.hdf5\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0482 - sparse_categorical_accuracy: 0.8063 - val_loss: 0.0539 - val_sparse_categorical_accuracy: 0.7814 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0473 - sparse_categorical_accuracy: 0.8128\n",
      "Epoch 24: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.8115 - val_loss: 0.0588 - val_sparse_categorical_accuracy: 0.7852 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0478 - sparse_categorical_accuracy: 0.8100\n",
      "Epoch 25: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.0544 - val_sparse_categorical_accuracy: 0.7885 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0471 - sparse_categorical_accuracy: 0.8151\n",
      "Epoch 26: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.8136 - val_loss: 0.0571 - val_sparse_categorical_accuracy: 0.7736 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0454 - sparse_categorical_accuracy: 0.8176\n",
      "Epoch 27: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0452 - sparse_categorical_accuracy: 0.8180 - val_loss: 0.0575 - val_sparse_categorical_accuracy: 0.7718 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0460 - sparse_categorical_accuracy: 0.8185\n",
      "Epoch 28: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.0598 - val_sparse_categorical_accuracy: 0.7775 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0465 - sparse_categorical_accuracy: 0.8102\n",
      "Epoch 29: val_loss did not improve from 0.05392\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.8099 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.7706 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0454 - sparse_categorical_accuracy: 0.8213\n",
      "Epoch 30: val_loss improved from 0.05392 to 0.05142, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch30-loss0.051423-acc0.802.hdf5\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0452 - sparse_categorical_accuracy: 0.8219 - val_loss: 0.0514 - val_sparse_categorical_accuracy: 0.8015 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0441 - sparse_categorical_accuracy: 0.8221\n",
      "Epoch 31: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0444 - sparse_categorical_accuracy: 0.8194 - val_loss: 0.0592 - val_sparse_categorical_accuracy: 0.7652 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0441 - sparse_categorical_accuracy: 0.8175\n",
      "Epoch 32: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.8194 - val_loss: 0.0535 - val_sparse_categorical_accuracy: 0.7850 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0452 - sparse_categorical_accuracy: 0.8142\n",
      "Epoch 33: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.8162 - val_loss: 0.0544 - val_sparse_categorical_accuracy: 0.7863 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0430 - sparse_categorical_accuracy: 0.8271\n",
      "Epoch 34: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0431 - sparse_categorical_accuracy: 0.8269 - val_loss: 0.0626 - val_sparse_categorical_accuracy: 0.7816 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0447 - sparse_categorical_accuracy: 0.8212\n",
      "Epoch 35: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.8223 - val_loss: 0.0608 - val_sparse_categorical_accuracy: 0.7685 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0433 - sparse_categorical_accuracy: 0.8265\n",
      "Epoch 36: val_loss did not improve from 0.05142\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0431 - sparse_categorical_accuracy: 0.8271 - val_loss: 0.0540 - val_sparse_categorical_accuracy: 0.7777 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0417 - sparse_categorical_accuracy: 0.8354\n",
      "Epoch 37: val_loss improved from 0.05142 to 0.04934, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch37-loss0.049337-acc0.807.hdf5\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.0493 - val_sparse_categorical_accuracy: 0.8070 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0439 - sparse_categorical_accuracy: 0.8279\n",
      "Epoch 38: val_loss did not improve from 0.04934\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0438 - sparse_categorical_accuracy: 0.8275 - val_loss: 0.0556 - val_sparse_categorical_accuracy: 0.8032 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0425 - sparse_categorical_accuracy: 0.8350\n",
      "Epoch 39: val_loss did not improve from 0.04934\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0427 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.0522 - val_sparse_categorical_accuracy: 0.7860 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0427 - sparse_categorical_accuracy: 0.8337\n",
      "Epoch 40: val_loss did not improve from 0.04934\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0426 - sparse_categorical_accuracy: 0.8335 - val_loss: 0.0546 - val_sparse_categorical_accuracy: 0.7905 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0419 - sparse_categorical_accuracy: 0.8319\n",
      "Epoch 41: val_loss improved from 0.04934 to 0.04909, saving model to /massimal/data/Larvik_Olberg/Hyperspectral/20210825/OlbergAreaS/X_SavedKerasModels/InpaintedDataset_TestImages-12-17-24/unet_model.epoch41-loss0.049091-acc0.811.hdf5\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0418 - sparse_categorical_accuracy: 0.8326 - val_loss: 0.0491 - val_sparse_categorical_accuracy: 0.8107 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0417 - sparse_categorical_accuracy: 0.8343\n",
      "Epoch 42: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.8345 - val_loss: 0.0516 - val_sparse_categorical_accuracy: 0.7772 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0416 - sparse_categorical_accuracy: 0.8318\n",
      "Epoch 43: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.8323 - val_loss: 0.0514 - val_sparse_categorical_accuracy: 0.7920 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0410 - sparse_categorical_accuracy: 0.8391\n",
      "Epoch 44: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.8383 - val_loss: 0.0527 - val_sparse_categorical_accuracy: 0.7980 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0415 - sparse_categorical_accuracy: 0.8373\n",
      "Epoch 45: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.8378 - val_loss: 0.0570 - val_sparse_categorical_accuracy: 0.7966 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0409 - sparse_categorical_accuracy: 0.8396\n",
      "Epoch 46: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0411 - sparse_categorical_accuracy: 0.8395 - val_loss: 0.0517 - val_sparse_categorical_accuracy: 0.8054 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0404 - sparse_categorical_accuracy: 0.8374\n",
      "Epoch 47: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.8377 - val_loss: 0.0502 - val_sparse_categorical_accuracy: 0.8095 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0421 - sparse_categorical_accuracy: 0.8286\n",
      "Epoch 48: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.8311 - val_loss: 0.0538 - val_sparse_categorical_accuracy: 0.8005 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0411 - sparse_categorical_accuracy: 0.8340\n",
      "Epoch 49: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0409 - sparse_categorical_accuracy: 0.8355 - val_loss: 0.0493 - val_sparse_categorical_accuracy: 0.8053 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0413 - sparse_categorical_accuracy: 0.8396\n",
      "Epoch 50: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.8382 - val_loss: 0.0545 - val_sparse_categorical_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0407 - sparse_categorical_accuracy: 0.8354\n",
      "Epoch 51: val_loss did not improve from 0.04909\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0411 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.0577 - val_sparse_categorical_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0395 - sparse_categorical_accuracy: 0.8421\n",
      "Epoch 52: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0396 - sparse_categorical_accuracy: 0.8416 - val_loss: 0.0513 - val_sparse_categorical_accuracy: 0.8037 - lr: 2.0000e-05\n",
      "Epoch 53/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0389 - sparse_categorical_accuracy: 0.8413\n",
      "Epoch 53: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.8396 - val_loss: 0.0519 - val_sparse_categorical_accuracy: 0.8069 - lr: 2.0000e-05\n",
      "Epoch 54/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0379 - sparse_categorical_accuracy: 0.8522\n",
      "Epoch 54: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.8519 - val_loss: 0.0517 - val_sparse_categorical_accuracy: 0.8032 - lr: 2.0000e-05\n",
      "Epoch 55/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0378 - sparse_categorical_accuracy: 0.8498\n",
      "Epoch 55: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0377 - sparse_categorical_accuracy: 0.8507 - val_loss: 0.0528 - val_sparse_categorical_accuracy: 0.7980 - lr: 2.0000e-05\n",
      "Epoch 56/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0380 - sparse_categorical_accuracy: 0.8506\n",
      "Epoch 56: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0377 - sparse_categorical_accuracy: 0.8514 - val_loss: 0.0534 - val_sparse_categorical_accuracy: 0.7992 - lr: 2.0000e-05\n",
      "Epoch 57/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0367 - sparse_categorical_accuracy: 0.8540\n",
      "Epoch 57: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0364 - sparse_categorical_accuracy: 0.8552 - val_loss: 0.0526 - val_sparse_categorical_accuracy: 0.7946 - lr: 2.0000e-05\n",
      "Epoch 58/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0373 - sparse_categorical_accuracy: 0.8479\n",
      "Epoch 58: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.8479 - val_loss: 0.0530 - val_sparse_categorical_accuracy: 0.8041 - lr: 2.0000e-05\n",
      "Epoch 59/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0382 - sparse_categorical_accuracy: 0.8468\n",
      "Epoch 59: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.8463 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8030 - lr: 2.0000e-05\n",
      "Epoch 60/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0371 - sparse_categorical_accuracy: 0.8506\n",
      "Epoch 60: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.8511 - val_loss: 0.0534 - val_sparse_categorical_accuracy: 0.8043 - lr: 2.0000e-05\n",
      "Epoch 61/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0376 - sparse_categorical_accuracy: 0.8489\n",
      "Epoch 61: val_loss did not improve from 0.04909\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0375 - sparse_categorical_accuracy: 0.8500 - val_loss: 0.0519 - val_sparse_categorical_accuracy: 0.8034 - lr: 2.0000e-05\n",
      "Epoch 62/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0374 - sparse_categorical_accuracy: 0.8520\n",
      "Epoch 62: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.0514 - val_sparse_categorical_accuracy: 0.8052 - lr: 4.0000e-06\n",
      "Epoch 63/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0362 - sparse_categorical_accuracy: 0.8537\n",
      "Epoch 63: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.8548 - val_loss: 0.0517 - val_sparse_categorical_accuracy: 0.8030 - lr: 4.0000e-06\n",
      "Epoch 64/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0370 - sparse_categorical_accuracy: 0.8569\n",
      "Epoch 64: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.8563 - val_loss: 0.0523 - val_sparse_categorical_accuracy: 0.8014 - lr: 4.0000e-06\n",
      "Epoch 65/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0370 - sparse_categorical_accuracy: 0.8494\n",
      "Epoch 65: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.8513 - val_loss: 0.0523 - val_sparse_categorical_accuracy: 0.8013 - lr: 4.0000e-06\n",
      "Epoch 66/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0366 - sparse_categorical_accuracy: 0.8565\n",
      "Epoch 66: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.8563 - val_loss: 0.0521 - val_sparse_categorical_accuracy: 0.8019 - lr: 4.0000e-06\n",
      "Epoch 67/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0372 - sparse_categorical_accuracy: 0.8535\n",
      "Epoch 67: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.0525 - val_sparse_categorical_accuracy: 0.8003 - lr: 4.0000e-06\n",
      "Epoch 68/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0375 - sparse_categorical_accuracy: 0.8494\n",
      "Epoch 68: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8014 - lr: 4.0000e-06\n",
      "Epoch 69/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0364 - sparse_categorical_accuracy: 0.8549\n",
      "Epoch 69: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.8552 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8013 - lr: 4.0000e-06\n",
      "Epoch 70/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0368 - sparse_categorical_accuracy: 0.8507\n",
      "Epoch 70: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8015 - lr: 4.0000e-06\n",
      "Epoch 71/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0370 - sparse_categorical_accuracy: 0.8525\n",
      "Epoch 71: val_loss did not improve from 0.04909\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.8545 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.7985 - lr: 4.0000e-06\n",
      "Epoch 72/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0374 - sparse_categorical_accuracy: 0.8536\n",
      "Epoch 72: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0375 - sparse_categorical_accuracy: 0.8523 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.7999 - lr: 8.0000e-07\n",
      "Epoch 73/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.8567\n",
      "Epoch 73: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.8568 - val_loss: 0.0528 - val_sparse_categorical_accuracy: 0.7988 - lr: 8.0000e-07\n",
      "Epoch 74/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0365 - sparse_categorical_accuracy: 0.8536\n",
      "Epoch 74: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0364 - sparse_categorical_accuracy: 0.8537 - val_loss: 0.0528 - val_sparse_categorical_accuracy: 0.7993 - lr: 8.0000e-07\n",
      "Epoch 75/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0370 - sparse_categorical_accuracy: 0.8521\n",
      "Epoch 75: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.0527 - val_sparse_categorical_accuracy: 0.8005 - lr: 8.0000e-07\n",
      "Epoch 76/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0357 - sparse_categorical_accuracy: 0.8608\n",
      "Epoch 76: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0360 - sparse_categorical_accuracy: 0.8602 - val_loss: 0.0528 - val_sparse_categorical_accuracy: 0.7993 - lr: 8.0000e-07\n",
      "Epoch 77/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.8571\n",
      "Epoch 77: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.8581 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8005 - lr: 8.0000e-07\n",
      "Epoch 78/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0375 - sparse_categorical_accuracy: 0.8516\n",
      "Epoch 78: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.8522 - val_loss: 0.0522 - val_sparse_categorical_accuracy: 0.8007 - lr: 8.0000e-07\n",
      "Epoch 79/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0372 - sparse_categorical_accuracy: 0.8566\n",
      "Epoch 79: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.0523 - val_sparse_categorical_accuracy: 0.7999 - lr: 8.0000e-07\n",
      "Epoch 80/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.8566\n",
      "Epoch 80: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 33ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.8569 - val_loss: 0.0529 - val_sparse_categorical_accuracy: 0.7976 - lr: 8.0000e-07\n",
      "Epoch 81/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0375 - sparse_categorical_accuracy: 0.8532\n",
      "Epoch 81: val_loss did not improve from 0.04909\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "115/115 [==============================] - 4s 34ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.7998 - lr: 8.0000e-07\n",
      "Epoch 82/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0373 - sparse_categorical_accuracy: 0.8507\n",
      "Epoch 82: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.0523 - val_sparse_categorical_accuracy: 0.8023 - lr: 1.6000e-07\n",
      "Epoch 83/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0356 - sparse_categorical_accuracy: 0.8613\n",
      "Epoch 83: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.8596 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.8023 - lr: 1.6000e-07\n",
      "Epoch 84/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.8539\n",
      "Epoch 84: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.8530 - val_loss: 0.0527 - val_sparse_categorical_accuracy: 0.7998 - lr: 1.6000e-07\n",
      "Epoch 85/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0374 - sparse_categorical_accuracy: 0.8484\n",
      "Epoch 85: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 32ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.8478 - val_loss: 0.0525 - val_sparse_categorical_accuracy: 0.8001 - lr: 1.6000e-07\n",
      "Epoch 86/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0369 - sparse_categorical_accuracy: 0.8524\n",
      "Epoch 86: val_loss did not improve from 0.04909\n",
      "115/115 [==============================] - 4s 31ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.0526 - val_sparse_categorical_accuracy: 0.8002 - lr: 1.6000e-07\n",
      "Epoch 87/200\n",
      " 61/115 [==============>...............] - ETA: 1s - loss: 0.0373 - sparse_categorical_accuracy: 0.8531"
     ]
    }
   ],
   "source": [
    "# Fit model to dataset\n",
    "history = unet.fit(train_dataset.batch(BATCH_SIZE),\n",
    "                   epochs=200,\n",
    "                   validation_data=val_dataset.batch(BATCH_SIZE),\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb397e52-ce53-421f-a7c5-87dfffb4e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, history.params['epochs'] + 1)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466fe03-6d29-4850-8d4c-a7fc5dd45a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, history.params['epochs'] + 1)\n",
    "train_acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
