{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee1bbf5-cf5f-4c51-b23c-201284661c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable TensorFlow debugging info and warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 2: Info and warnings not displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f336636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add massimal tools folder to path\n",
    "import sys\n",
    "sys.path.append(\"/massimal/python/tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86d6258-75d4-4eae-a919-16da06175e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import pickle\n",
    "import hyspec_cnn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69c67e6-0662-4bfd-a4af-db2f0c22acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_dir = pathlib.Path('/massimal/data/Vega_Sola/Hyperspectral/20220823/Area')\n",
    "train_tiles_path = base_dir / '3a_PCA_TrainValidationSplit/Training/PCA-Tiles/20220823_Vega_Sola_Train_Tiles'\n",
    "val_tiles_path = base_dir / '3a_PCA_TrainValidationSplit/Validation/PCA-Tiles/20220823_Vega_Sola_Val_Tiles'\n",
    "\n",
    "unet_model_save_dir = base_dir / 'M_UnetModels'\n",
    "unet_model_save_dir.mkdir(exist_ok=True)\n",
    "tensorboard_log_dir = base_dir / 'M_TensorBoardLogs'\n",
    "tensorboard_log_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9740aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is used\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800c525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "OUTPUT_CHANNELS = 8\n",
    "BATCH_SIZE = 8\n",
    "DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc79a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets \n",
    "train_dataset = tf.data.Dataset.load(str(train_tiles_path))\n",
    "val_dataset = tf.data.Dataset.load(str(val_tiles_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d6eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tiles: 3266\n",
      "Number of validation tiles: 711\n",
      "Tile data shape (PCA tiles): (128, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Get number of tiles in each dataset, and dataset shape\n",
    "n_tiles_train = train_dataset.cardinality()\n",
    "n_tiles_val = val_dataset.cardinality()\n",
    "tile_nrows,tile_ncols,tile_nchannels = train_dataset.element_spec[0].shape.as_list()\n",
    "print(f'Number of training tiles: {n_tiles_train}')\n",
    "print(f'Number of validation tiles: {n_tiles_val}')\n",
    "print(f'Tile data shape (PCA tiles): {(tile_nrows,tile_ncols,tile_nchannels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b186e2db-c55d-45c1-8829-33560a754ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "# {'Rock': 0.89,\n",
    "#  'Cobble': 1.24,\n",
    "#  'Sand': 0.82,\n",
    "#  'Mearl bed': 0.65,\n",
    "#  'Rockweed': 1.08,\n",
    "#  'Kelp': 1.54,\n",
    "#  'Brown algae': 0.77}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e694020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/tutorials/images/segmentation#optional_imbalanced_classes_and_class_weights\n",
    "def add_sample_weights(image, label, name):\n",
    "    # class_weights = tf.constant([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]) # Hard-coded for 7 classes\n",
    "    class_weights = tf.constant([0.0, 0.89, 1.24, 0.82, 0.65, 1.08, 1.54, 0.77]) # Hard-coded for 7 classes\n",
    "    class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe104df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training dataset (tiles are originally ordered by image) and add sample weights\n",
    "train_dataset = train_dataset.shuffle(buffer_size=n_tiles_train)\n",
    "train_dataset = train_dataset.map(add_sample_weights)\n",
    "val_dataset = val_dataset.map(add_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15678642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch datasets\n",
    "train_dataset_batch = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset_batch = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66b181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, None, None,  0           []                               \n",
      "                                 8)]                                                              \n",
      "                                                                                                  \n",
      " augmentation (Sequential)      (None, None, None,   0           ['input_image[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " initial_convolution (Conv2D)   (None, None, None,   2336        ['augmentation[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " downsamp_res_1/2 (Sequential)  (None, None, None,   33024       ['initial_convolution[0][0]']    \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " downsamp_res_1/4 (Sequential)  (None, None, None,   131584      ['downsamp_res_1/2[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " upsamp_res_1/2 (Sequential)    (None, None, None,   131328      ['downsamp_res_1/4[0][0]']       \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " skipconnection_res_1/2 (Concat  (None, None, None,   0          ['upsamp_res_1/2[0][0]',         \n",
      " enate)                         128)                              'downsamp_res_1/2[0][0]']       \n",
      "                                                                                                  \n",
      " upsamp_res_1/1 (Sequential)    (None, None, None,   131328      ['skipconnection_res_1/2[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " skipconnection_res_1/1 (Concat  (None, None, None,   0          ['upsamp_res_1/1[0][0]',         \n",
      " enate)                         96)                               'initial_convolution[0][0]']    \n",
      "                                                                                                  \n",
      " classification (Conv2D)        (None, None, None,   6920        ['skipconnection_res_1/1[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 436,520\n",
      "Trainable params: 435,880\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "unet = hyspec_cnn.unet(input_channels=tile_nchannels,\n",
    "                       output_channels=OUTPUT_CHANNELS,\n",
    "                       first_layer_channels=32,\n",
    "                       depth = DEPTH,\n",
    "               )\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264c87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "input_image\n",
      "----\n",
      "augmentation\n",
      "\trandom_flip\n",
      "----\n",
      "initial_convolution\n",
      "----\n",
      "downsamp_res_1/2\n",
      "\tconv2d\n",
      "\tbatch_normalization\n",
      "\tleaky_re_lu\n",
      "----\n",
      "downsamp_res_1/4\n",
      "\tconv2d_1\n",
      "\tbatch_normalization_1\n",
      "\tleaky_re_lu_1\n",
      "----\n",
      "upsamp_res_1/2\n",
      "\tconv2d_transpose\n",
      "\tbatch_normalization_2\n",
      "\tre_lu\n",
      "----\n",
      "skipconnection_res_1/2\n",
      "----\n",
      "upsamp_res_1/1\n",
      "\tconv2d_transpose_1\n",
      "\tbatch_normalization_3\n",
      "\tre_lu_1\n",
      "----\n",
      "skipconnection_res_1/1\n",
      "----\n",
      "classification\n"
     ]
    }
   ],
   "source": [
    "# Print layers with sublayers\n",
    "for layer in unet.layers:\n",
    "    print('----')\n",
    "    print(layer.name)\n",
    "    if hasattr(layer,'layers'):\n",
    "        for l in layer.layers:\n",
    "            print('\\t'+l.name)\n",
    "       # print(layer.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f909d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "model_save_filename = str(unet_model_save_dir) + '/unet_model.depth' + str(DEPTH) +'.epoch{epoch:02d}-loss{val_loss:.6f}-acc{val_sparse_categorical_accuracy:.3f}.hdf5'\n",
    "callbacks =[tf.keras.callbacks.ModelCheckpoint(filepath = model_save_filename,\n",
    "                                               save_best_only=True,\n",
    "                                               verbose = 1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, verbose=1),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir= tensorboard_log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "847ab21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch{epoch:02d}-loss{val_loss:.6f}-acc{val_sparse_categorical_accuracy:.3f}.hdf5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05a527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "unet.compile(optimizer=tf.keras.optimizers.RMSprop(0.0001), \n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             weighted_metrics=['sparse_categorical_accuracy'], # Need weights to ignore background\n",
    "             metrics = []) # Sparse because classes are numbered, not one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6beeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensorboard_log_dir)\n",
    "#%tensorboard --logdir /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_TensorBoardLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc9cd987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "409/409 [==============================] - ETA: 0s - loss: 0.0897 - sparse_categorical_accuracy: 0.5485\n",
      "Epoch 1: val_loss improved from inf to 0.06365, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch01-loss0.063652-acc0.664.hdf5\n",
      "409/409 [==============================] - 65s 118ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.5485 - val_loss: 0.0637 - val_sparse_categorical_accuracy: 0.6645 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0618 - sparse_categorical_accuracy: 0.6754\n",
      "Epoch 2: val_loss improved from 0.06365 to 0.05732, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch02-loss0.057322-acc0.703.hdf5\n",
      "409/409 [==============================] - 54s 112ms/step - loss: 0.0618 - sparse_categorical_accuracy: 0.6754 - val_loss: 0.0573 - val_sparse_categorical_accuracy: 0.7032 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0546 - sparse_categorical_accuracy: 0.7176\n",
      "Epoch 3: val_loss improved from 0.05732 to 0.05327, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch03-loss0.053272-acc0.724.hdf5\n",
      "409/409 [==============================] - 52s 113ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.7173 - val_loss: 0.0533 - val_sparse_categorical_accuracy: 0.7237 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0506 - sparse_categorical_accuracy: 0.7382\n",
      "Epoch 4: val_loss improved from 0.05327 to 0.04733, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch04-loss0.047329-acc0.769.hdf5\n",
      "409/409 [==============================] - 48s 114ms/step - loss: 0.0506 - sparse_categorical_accuracy: 0.7382 - val_loss: 0.0473 - val_sparse_categorical_accuracy: 0.7691 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0479 - sparse_categorical_accuracy: 0.7544\n",
      "Epoch 5: val_loss improved from 0.04733 to 0.04592, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch05-loss0.045922-acc0.771.hdf5\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.7544 - val_loss: 0.0459 - val_sparse_categorical_accuracy: 0.7706 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0463 - sparse_categorical_accuracy: 0.7602\n",
      "Epoch 6: val_loss improved from 0.04592 to 0.04340, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch06-loss0.043402-acc0.779.hdf5\n",
      "409/409 [==============================] - 47s 112ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.7603 - val_loss: 0.0434 - val_sparse_categorical_accuracy: 0.7787 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0441 - sparse_categorical_accuracy: 0.7693\n",
      "Epoch 7: val_loss did not improve from 0.04340\n",
      "409/409 [==============================] - 48s 115ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.7691 - val_loss: 0.0458 - val_sparse_categorical_accuracy: 0.7544 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0422 - sparse_categorical_accuracy: 0.7809\n",
      "Epoch 8: val_loss improved from 0.04340 to 0.04126, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch08-loss0.041258-acc0.789.hdf5\n",
      "409/409 [==============================] - 49s 116ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.7811 - val_loss: 0.0413 - val_sparse_categorical_accuracy: 0.7888 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0401 - sparse_categorical_accuracy: 0.7920\n",
      "Epoch 9: val_loss did not improve from 0.04126\n",
      "409/409 [==============================] - 48s 115ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.7921 - val_loss: 0.0425 - val_sparse_categorical_accuracy: 0.7655 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0394 - sparse_categorical_accuracy: 0.7963\n",
      "Epoch 10: val_loss improved from 0.04126 to 0.04109, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch10-loss0.041093-acc0.780.hdf5\n",
      "409/409 [==============================] - 48s 114ms/step - loss: 0.0395 - sparse_categorical_accuracy: 0.7962 - val_loss: 0.0411 - val_sparse_categorical_accuracy: 0.7802 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0387 - sparse_categorical_accuracy: 0.7971\n",
      "Epoch 11: val_loss did not improve from 0.04109\n",
      "409/409 [==============================] - 47s 112ms/step - loss: 0.0388 - sparse_categorical_accuracy: 0.7970 - val_loss: 0.0444 - val_sparse_categorical_accuracy: 0.7518 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0382 - sparse_categorical_accuracy: 0.8018\n",
      "Epoch 12: val_loss did not improve from 0.04109\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0383 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.0429 - val_sparse_categorical_accuracy: 0.7812 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0376 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 13: val_loss did not improve from 0.04109\n",
      "409/409 [==============================] - 47s 110ms/step - loss: 0.0377 - sparse_categorical_accuracy: 0.8059 - val_loss: 0.0442 - val_sparse_categorical_accuracy: 0.7828 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0358 - sparse_categorical_accuracy: 0.8155\n",
      "Epoch 14: val_loss improved from 0.04109 to 0.03907, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch14-loss0.039074-acc0.821.hdf5\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.8155 - val_loss: 0.0391 - val_sparse_categorical_accuracy: 0.8213 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0360 - sparse_categorical_accuracy: 0.8117\n",
      "Epoch 15: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.0401 - val_sparse_categorical_accuracy: 0.7963 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0346 - sparse_categorical_accuracy: 0.8214\n",
      "Epoch 16: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 47s 110ms/step - loss: 0.0346 - sparse_categorical_accuracy: 0.8214 - val_loss: 0.0406 - val_sparse_categorical_accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0348 - sparse_categorical_accuracy: 0.8209\n",
      "Epoch 17: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 49s 110ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.8208 - val_loss: 0.0424 - val_sparse_categorical_accuracy: 0.7935 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0341 - sparse_categorical_accuracy: 0.8223\n",
      "Epoch 18: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.8224 - val_loss: 0.0489 - val_sparse_categorical_accuracy: 0.7206 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0337 - sparse_categorical_accuracy: 0.8237\n",
      "Epoch 19: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 109ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.0401 - val_sparse_categorical_accuracy: 0.7920 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0328 - sparse_categorical_accuracy: 0.8283\n",
      "Epoch 20: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.8281 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.8102 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0329 - sparse_categorical_accuracy: 0.8284\n",
      "Epoch 21: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 109ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.8284 - val_loss: 0.0408 - val_sparse_categorical_accuracy: 0.7959 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0317 - sparse_categorical_accuracy: 0.8345\n",
      "Epoch 22: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.8344 - val_loss: 0.0471 - val_sparse_categorical_accuracy: 0.7423 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0318 - sparse_categorical_accuracy: 0.8338\n",
      "Epoch 23: val_loss did not improve from 0.03907\n",
      "409/409 [==============================] - 46s 109ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.8338 - val_loss: 0.0466 - val_sparse_categorical_accuracy: 0.7666 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0315 - sparse_categorical_accuracy: 0.8367\n",
      "Epoch 24: val_loss did not improve from 0.03907\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.0472 - val_sparse_categorical_accuracy: 0.7699 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0285 - sparse_categorical_accuracy: 0.8534\n",
      "Epoch 25: val_loss improved from 0.03907 to 0.03738, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch25-loss0.037376-acc0.812.hdf5\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.8535 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.8123 - lr: 2.0000e-05\n",
      "Epoch 26/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0280 - sparse_categorical_accuracy: 0.8539\n",
      "Epoch 26: val_loss did not improve from 0.03738\n",
      "409/409 [==============================] - 47s 109ms/step - loss: 0.0280 - sparse_categorical_accuracy: 0.8539 - val_loss: 0.0400 - val_sparse_categorical_accuracy: 0.8042 - lr: 2.0000e-05\n",
      "Epoch 27/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0279 - sparse_categorical_accuracy: 0.8559\n",
      "Epoch 27: val_loss did not improve from 0.03738\n",
      "409/409 [==============================] - 46s 109ms/step - loss: 0.0279 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.8186 - lr: 2.0000e-05\n",
      "Epoch 28/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0277 - sparse_categorical_accuracy: 0.8561\n",
      "Epoch 28: val_loss improved from 0.03738 to 0.03707, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch28-loss0.037070-acc0.829.hdf5\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.8562 - val_loss: 0.0371 - val_sparse_categorical_accuracy: 0.8289 - lr: 2.0000e-05\n",
      "Epoch 29/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0273 - sparse_categorical_accuracy: 0.8594\n",
      "Epoch 29: val_loss improved from 0.03707 to 0.03692, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch29-loss0.036918-acc0.827.hdf5\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.8595 - val_loss: 0.0369 - val_sparse_categorical_accuracy: 0.8272 - lr: 2.0000e-05\n",
      "Epoch 30/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0274 - sparse_categorical_accuracy: 0.8584\n",
      "Epoch 30: val_loss improved from 0.03692 to 0.03640, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch30-loss0.036398-acc0.837.hdf5\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.8585 - val_loss: 0.0364 - val_sparse_categorical_accuracy: 0.8365 - lr: 2.0000e-05\n",
      "Epoch 31/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0274 - sparse_categorical_accuracy: 0.8581\n",
      "Epoch 31: val_loss improved from 0.03640 to 0.03628, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch31-loss0.036282-acc0.825.hdf5\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.8579 - val_loss: 0.0363 - val_sparse_categorical_accuracy: 0.8251 - lr: 2.0000e-05\n",
      "Epoch 32/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0268 - sparse_categorical_accuracy: 0.8607\n",
      "Epoch 32: val_loss did not improve from 0.03628\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.8607 - val_loss: 0.0373 - val_sparse_categorical_accuracy: 0.8267 - lr: 2.0000e-05\n",
      "Epoch 33/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0267 - sparse_categorical_accuracy: 0.8625\n",
      "Epoch 33: val_loss did not improve from 0.03628\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.8624 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.8246 - lr: 2.0000e-05\n",
      "Epoch 34/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0268 - sparse_categorical_accuracy: 0.8628\n",
      "Epoch 34: val_loss improved from 0.03628 to 0.03620, saving model to /massimal/data/Vega_Sola/Hyperspectral/20220823/Area/M_UnetModels/unet_model.depth2.epoch34-loss0.036197-acc0.834.hdf5\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.8628 - val_loss: 0.0362 - val_sparse_categorical_accuracy: 0.8340 - lr: 2.0000e-05\n",
      "Epoch 35/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0264 - sparse_categorical_accuracy: 0.8640\n",
      "Epoch 35: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 46s 110ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.8640 - val_loss: 0.0389 - val_sparse_categorical_accuracy: 0.8197 - lr: 2.0000e-05\n",
      "Epoch 36/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0268 - sparse_categorical_accuracy: 0.8629\n",
      "Epoch 36: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.8630 - val_loss: 0.0364 - val_sparse_categorical_accuracy: 0.8287 - lr: 2.0000e-05\n",
      "Epoch 37/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0263 - sparse_categorical_accuracy: 0.8643\n",
      "Epoch 37: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.8643 - val_loss: 0.0388 - val_sparse_categorical_accuracy: 0.8163 - lr: 2.0000e-05\n",
      "Epoch 38/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0258 - sparse_categorical_accuracy: 0.8682\n",
      "Epoch 38: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.8681 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.8196 - lr: 2.0000e-05\n",
      "Epoch 39/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0263 - sparse_categorical_accuracy: 0.8629\n",
      "Epoch 39: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.0383 - val_sparse_categorical_accuracy: 0.8213 - lr: 2.0000e-05\n",
      "Epoch 40/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0262 - sparse_categorical_accuracy: 0.8663\n",
      "Epoch 40: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.8665 - val_loss: 0.0366 - val_sparse_categorical_accuracy: 0.8352 - lr: 2.0000e-05\n",
      "Epoch 41/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0266 - sparse_categorical_accuracy: 0.8622\n",
      "Epoch 41: val_loss did not improve from 0.03620\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.0412 - val_sparse_categorical_accuracy: 0.8048 - lr: 2.0000e-05\n",
      "Epoch 42/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0254 - sparse_categorical_accuracy: 0.8680\n",
      "Epoch 42: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.0371 - val_sparse_categorical_accuracy: 0.8317 - lr: 4.0000e-06\n",
      "Epoch 43/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0249 - sparse_categorical_accuracy: 0.8723\n",
      "Epoch 43: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.8723 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.8346 - lr: 4.0000e-06\n",
      "Epoch 44/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0254 - sparse_categorical_accuracy: 0.8701\n",
      "Epoch 44: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.8702 - val_loss: 0.0367 - val_sparse_categorical_accuracy: 0.8364 - lr: 4.0000e-06\n",
      "Epoch 45/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0256 - sparse_categorical_accuracy: 0.8687\n",
      "Epoch 45: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.8337 - lr: 4.0000e-06\n",
      "Epoch 46/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0253 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 46: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.8300 - lr: 4.0000e-06\n",
      "Epoch 47/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0247 - sparse_categorical_accuracy: 0.8740\n",
      "Epoch 47: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.8739 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.8354 - lr: 4.0000e-06\n",
      "Epoch 48/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0251 - sparse_categorical_accuracy: 0.8715\n",
      "Epoch 48: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.8710 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.8294 - lr: 4.0000e-06\n",
      "Epoch 49/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.8711\n",
      "Epoch 49: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.8711 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.8299 - lr: 4.0000e-06\n",
      "Epoch 50/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0249 - sparse_categorical_accuracy: 0.8721\n",
      "Epoch 50: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.8716 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.8280 - lr: 4.0000e-06\n",
      "Epoch 51/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0250 - sparse_categorical_accuracy: 0.8715\n",
      "Epoch 51: val_loss did not improve from 0.03620\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.8713 - val_loss: 0.0385 - val_sparse_categorical_accuracy: 0.8283 - lr: 4.0000e-06\n",
      "Epoch 52/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.8738\n",
      "Epoch 52: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.8739 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.8273 - lr: 8.0000e-07\n",
      "Epoch 53/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0250 - sparse_categorical_accuracy: 0.8726\n",
      "Epoch 53: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.8726 - val_loss: 0.0379 - val_sparse_categorical_accuracy: 0.8303 - lr: 8.0000e-07\n",
      "Epoch 54/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.8714\n",
      "Epoch 54: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.8714 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.8299 - lr: 8.0000e-07\n",
      "Epoch 55/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.8734\n",
      "Epoch 55: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 111ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.8732 - val_loss: 0.0377 - val_sparse_categorical_accuracy: 0.8311 - lr: 8.0000e-07\n",
      "Epoch 56/100\n",
      "408/409 [============================>.] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.8723\n",
      "Epoch 56: val_loss did not improve from 0.03620\n",
      "409/409 [==============================] - 47s 112ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.8722 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.8309 - lr: 8.0000e-07\n",
      "Epoch 57/100\n",
      "343/409 [========================>.....] - ETA: 6s - loss: 0.0245 - sparse_categorical_accuracy: 0.8763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit model to dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model to dataset\n",
    "history = unet.fit(train_dataset.batch(BATCH_SIZE),\n",
    "                   epochs=100,\n",
    "                   validation_data=val_dataset.batch(BATCH_SIZE),\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc82e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
